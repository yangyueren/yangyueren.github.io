<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yangyueren.github.io</id>
    <title>木头人</title>
    <updated>2021-02-27T06:43:26.435Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yangyueren.github.io"/>
    <link rel="self" href="https://yangyueren.github.io/atom.xml"/>
    <subtitle>每日一记</subtitle>
    <logo>https://yangyueren.github.io/images/avatar.png</logo>
    <icon>https://yangyueren.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, 木头人</rights>
    <entry>
        <title type="html"><![CDATA[MIT Missing Semester in CS Education : Shell Notes]]></title>
        <id>https://yangyueren.github.io/post/mit-missing-semester-in-cs-education-shell-notes/</id>
        <link href="https://yangyueren.github.io/post/mit-missing-semester-in-cs-education-shell-notes/">
        </link>
        <updated>2021-02-27T06:42:02.000Z</updated>
        <content type="html"><![CDATA[<h1 id="shell-and-useful-tools">shell and useful tools</h1>
<p>找shell命令的历史记录 ctrl-r 快捷键</p>
<p>字符串：<br>
'' ：原义字符串<br>
“” ：转义</p>
<pre><code>foo=bar
echo '$foo'  # $foo
echo &quot;$foo&quot;  # bar
</code></pre>
<p>练习：假设您有一个命令，它很少出错。因此为了在出错时能够对其进行调试，需要花费大量的时间重现错误并捕获输出。 编写一段bash脚本，运行如下的脚本直到它出错，将它的标准输出和标准错误流记录到文件，并在最后输出所有内容。 加分项：报告脚本在失败前共运行了多少次。</p>
<pre><code>#!/usr/bin/env bash

n=$(( RANDOM % 100 ))

if [[ n -eq 42 ]]; then
	echo &quot;Something went wrong&quot;
	&gt;&amp;2 echo &quot;The error was using magic numbers&quot;
	exit 1
fi

echo &quot;Everything went according to plan&quot;
</code></pre>
<p>solution</p>
<pre><code># !/bin/bash
#$0 - 脚本名
#$1 到 $9 - 脚本的参数。 $1 是第一个参数，依此类推。
#$@ - 所有参数
#$# - 参数个数
#$? - 前一个命令的返回值; 0 代表正常返回，其他非0返回表示有错误发生
#$$ - 当前脚本的进程识别码
#!! - 完整的上一条命令，包括参数。常见应用：当你因为权限不足执行命令失败时，可以使用 su    do !!再尝试一次。
file=&quot;failure.sh&quot;

./$file 1&gt;&gt;run.log 2&gt;&gt;error.log
# 1代表stdout 重定向到run.log

while [[ $? -eq 0 ]]; do
     ./$file 1&gt;&gt;run.log 2&gt;&gt;error.log
done

echo &quot;$0 finished&quot;
</code></pre>
<p>sshfd可以将远端服务器的一个文件夹挂载到本地，然后使用本地的编辑器了。</p>
<p>练习：<br>
如果您希望某个进程结束后再开始另外一个进程， 应该如何实现呢？在这个练习中，我们使用 sleep 60 &amp; 作为先执行的程序。一种方法是使用 wait 命令。尝试启动这个休眠命令，然后待其结束后再执行 ls 命令。<br>
但是，如果我们在不同的 bash 会话中进行操作，则上述方法就不起作用了。因为 wait 只能对子进程起作用。之前我们没有提过的一个特性是，kill 命令成功退出时其状态码为 0 ，其他状态则是非0。kill -0 则不会发送信号，但是会在进程不存在时返回一个不为0的状态码。请编写一个 bash 函数 pidwait ，它接受一个 pid 作为输入参数，然后一直等待直到该进程结束。您需要使用 sleep 来避免浪费 CPU 性能。</p>
<pre><code>#!/bin/bash
com=&quot;kill -0 $1&quot;
echo $com
$com
while [[ $? -eq 0 ]]; do
	echo &quot;$1 is still running&quot;
	sleep 60
done
echo &quot;$1 doesn't exist&quot;

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[前端 Vue-cli4.x项目执行顺序]]></title>
        <id>https://yangyueren.github.io/post/qian-duan-vue-cli4x-xiang-mu-zhi-xing-shun-xu/</id>
        <link href="https://yangyueren.github.io/post/qian-duan-vue-cli4x-xiang-mu-zhi-xing-shun-xu/">
        </link>
        <updated>2021-02-24T14:42:55.000Z</updated>
        <content type="html"><![CDATA[<p>由于要做一个可视化，选择了vue，但是在使用过程中不清楚vue的启动过程导致写出的代码不够满意，于是在此理顺vue的启动过程。</p>
<p>代码如下：</p>
<pre><code>src
--components
|----HelloWorld.vue
--router
|----index.js
--views
|--Home.vue
--App.vue
main.js
index.html
</code></pre>
<p>启动过程：main.js --&gt; App.vue --&gt; router/index.js --&gt; Home.vue --&gt; HelloWorld.vue</p>
<p>index.html是初始文件，vue会替换掉id=app的dom<br>
index.html</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta charset=&quot;utf-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1.0&quot;&gt;
    &lt;title&gt;detection_web_vue&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div id=&quot;app&quot;&gt;&lt;/div&gt;
    &lt;!-- built files will be auto injected --&gt;
  &lt;/body&gt;
&lt;/html&gt;

</code></pre>
<p>main.js是首先执行的文件，是入口函数，创建一个Vue，把App component绑定到#app的dom<br>
main.js</p>
<pre><code>// The Vue build version to load with the `import` command
// (runtime-only or standalone) has been set in webpack.base.conf with an alias.
import Vue from 'vue'
import App from './App'
import router from './router'

Vue.config.productionTip = false

/* eslint-disable no-new */
new Vue({
  el: '#app',
  router,
  components: { App },
  template: '&lt;App/&gt;'
})

</code></pre>
<p>这里的router-link会被渲染<br>
router-view也会被渲染，进入router/index.js中<br>
App.vue</p>
<pre><code>&lt;template&gt;
  &lt;div id=&quot;app&quot;&gt;
    &lt;img src=&quot;./assets/logo.png&quot;&gt;
    &lt;router-link to='/home'&gt; Home &lt;/router-link&gt;
    &lt;router-view/&gt;
  &lt;/div&gt;
&lt;/template&gt;

&lt;script&gt;
export default {
  name: 'App'
}
&lt;/script&gt;

&lt;style&gt;
#app {
  font-family: 'Avenir', Helvetica, Arial, sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  text-align: center;
  color: #2c3e50;
  margin-top: 60px;
}
&lt;/style&gt;

</code></pre>
<p>引入了HelloWorld组件，在Home中使用<br>
router/index.js</p>
<pre><code>import Vue from 'vue'
import Router from 'vue-router'
import HelloWorld from '@/components/HelloWorld'
import Home from '@/views/Home'

Vue.use(Router)

export default new Router({
  routes: [
    {
      path: '/home',
      name: 'Home',
      component: Home
    }
  ]
})

</code></pre>
<p>Home.vue</p>
<pre><code>&lt;template&gt;
  &lt;div class=&quot;hello&quot;&gt;
    &lt;h2&gt;Essential Links&lt;/h2&gt;
    &lt;HelloWorld/&gt;
  &lt;/div&gt;
&lt;/template&gt;

&lt;script&gt;
import HelloWorld from '@/components/HelloWorld.vue'
export default {
  name: 'Home',
  components: {
      HelloWorld
  }
}
&lt;/script&gt;

&lt;!-- Add &quot;scoped&quot; attribute to limit CSS to this component only --&gt;
&lt;style scoped&gt;
h1, h2 {
  font-weight: normal;
}
ul {
  list-style-type: none;
  padding: 0;
}
li {
  display: inline-block;
  margin: 0 10px;
}
a {
  color: #42b983;
}
&lt;/style&gt;

</code></pre>
<p>HelloWorld.vue</p>
<pre><code>&lt;template&gt;
  &lt;div class=&quot;hello&quot;&gt;
    &lt;h1&gt;{{ msg }}&lt;/h1&gt;
    &lt;h2&gt;Essential Links&lt;/h2&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;https://vuejs.org&quot;
          target=&quot;_blank&quot;
        &gt;
          Core Docs
        &lt;/a&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;https://forum.vuejs.org&quot;
          target=&quot;_blank&quot;
        &gt;
          Forum
        &lt;/a&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;https://chat.vuejs.org&quot;
          target=&quot;_blank&quot;
        &gt;
          Community Chat
        &lt;/a&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;https://twitter.com/vuejs&quot;
          target=&quot;_blank&quot;
        &gt;
          Twitter
        &lt;/a&gt;
      &lt;/li&gt;
      &lt;br&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;http://vuejs-templates.github.io/webpack/&quot;
          target=&quot;_blank&quot;
        &gt;
          Docs for This Template
        &lt;/a&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;h2&gt;Ecosystem&lt;/h2&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;http://router.vuejs.org/&quot;
          target=&quot;_blank&quot;
        &gt;
          vue-router
        &lt;/a&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;http://vuex.vuejs.org/&quot;
          target=&quot;_blank&quot;
        &gt;
          vuex
        &lt;/a&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;http://vue-loader.vuejs.org/&quot;
          target=&quot;_blank&quot;
        &gt;
          vue-loader
        &lt;/a&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;https://github.com/vuejs/awesome-vue&quot;
          target=&quot;_blank&quot;
        &gt;
          awesome-vue
        &lt;/a&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/div&gt;
&lt;/template&gt;

&lt;script&gt;
export default {
  name: 'HelloWorld',
  data () {
    return {
      msg: 'Welcome to Your Vue.js App'
    }
  }
}
&lt;/script&gt;

&lt;!-- Add &quot;scoped&quot; attribute to limit CSS to this component only --&gt;
&lt;style scoped&gt;
h1, h2 {
  font-weight: normal;
}
ul {
  list-style-type: none;
  padding: 0;
}
li {
  display: inline-block;
  margin: 0 10px;
}
a {
  color: #42b983;
}
&lt;/style&gt;

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSAPP RIO包用于I/O]]></title>
        <id>https://yangyueren.github.io/post/csapp-rio-bao-yong-yu-io/</id>
        <link href="https://yangyueren.github.io/post/csapp-rio-bao-yong-yu-io/">
        </link>
        <updated>2021-02-07T17:12:29.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<pre><code class="language-markdown">RIO

- 解决short count

- 提供方便、健壮、高效的IO

两类函数

- 无缓冲的输入输出

  rio_readn

  rio_writen

​		绝不会返回不足值

- 带缓冲的输入输出

​        rio_readlineb

​            读文本行

​            停止条件

​                读了maxlen个字节

​                遇到了EOF

​                遇到了换行符

​        rio_readnb

​            读字节 不区分文本和二进制文件

​            停止条件

​           	 maxlen : 因为里面用了一个while循环，如果还有left，就继续读，如果buffer已经空了，就调用rio_read填充buffer后再读。

​            	遇到EOF

​        

​        readlineb和readnb可以混用，但不能和readn混用
</code></pre>
<pre><code class="language-c">/*my version of rio*/
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;string.h&gt;
#include &lt;errno.h&gt;

typedef struct{
    int rio_fd; //file descriptor
    int rio_cnt; //unread bytes
    char rio_buffer[32];
    char *rio_p; //pointer to next unread byte;
} rio_t;

void rio_initb(rio_t *rp, int fd){
    rp-&gt;rio_fd = fd;
    rp-&gt;rio_cnt = 0;
    rp-&gt;rio_p = rp-&gt;rio_buffer;
}

//把buffer里的拷贝进usrbuf
int rio_read(rio_t *rp, char *usrbuf, size_t size){

    while(rp-&gt;rio_cnt &lt;= 0){
        rp-&gt;rio_cnt = read(rp-&gt;rio_fd, rp-&gt;rio_buffer, sizeof(rp-&gt;rio_buffer));
        if(rp-&gt;rio_cnt &lt; 0){
            if(errno != EINTR){
                return -1;
            }
        }else if(rp-&gt;rio_cnt == 0){
            return 0;
        }else{
            rp-&gt;rio_p = rp-&gt;rio_buffer;
        }
    }
    int cnt = size;
    if(cnt &gt; rp-&gt;rio_cnt){
        cnt = rp-&gt;rio_cnt;
    }
    memcpy(usrbuf, rp-&gt;rio_p, cnt);
    rp-&gt;rio_p += cnt;
    rp-&gt;rio_cnt -= cnt;
    return cnt;
}

int rio_readnb(rio_t *rp, char *des, size_t size){
    int left = size;
    int n;
    char *p = des;
    while(left &gt; 0){
        n = rio_read(rp, p, left);
        if(n &lt; 0){
            return -1; /* errno set by read() */
        }else if(n==0){
            break; /* EOF */
        }else{
            left -= n;
            p += n;
        }
    }
    return (size - left);
}

int rio_readlineb(rio_t *rp, char* des, size_t max_size){
    int left = max_size-1;
    char *p = des;
    int n;
    while(left &gt; 0){
        if(n = rio_read(rp, p, 1) == 1){
            left--;
            if(*p == '\n'){
                p++;
                break;
            }else{
                p++;
            }
            
        }else if(n==0){
            if(p == des) return 0; //no data read, EOF
            else break; // some data read, EOF
        }else{
            return -1;
        }
    }
    *p = 0;
    return p-des;
}

int main()
{
    rio_t rt;
    int fd1 = open(&quot;README2&quot;, O_RDONLY);
    rio_initb(&amp;rt, fd1);

    
    char buf[64];
    int size = 64;
    rio_readlineb(&amp;rt, buf, size);
    printf(&quot;%s&quot;, buf);

}
</code></pre>
<pre><code class="language-c">
/****************************************
 * The Rio package - Robust I/O functions
 ****************************************/

/*
 * rio_readn - Robustly read n bytes (unbuffered)
 */
/* $begin rio_readn */
ssize_t rio_readn(int fd, void *usrbuf, size_t n)
{
    size_t nleft = n;
    ssize_t nread;
    char *bufp = usrbuf;

    while (nleft &gt; 0)
    {
        if ((nread = read(fd, bufp, nleft)) &lt; 0)
        {
            if (errno == EINTR) /* Interrupted by sig handler return */
                nread = 0;      /* and call read() again */
            else
                return -1; /* errno set by read() */
        }
        else if (nread == 0)
            break; /* EOF */
        nleft -= nread;
        bufp += nread;
    }
    return (n - nleft); /* Return &gt;= 0 */
}
/* $end rio_readn */

/*
 * rio_writen - Robustly write n bytes (unbuffered)
 */
/* $begin rio_writen */
ssize_t rio_writen(int fd, void *usrbuf, size_t n)
{
    size_t nleft = n;
    ssize_t nwritten;
    char *bufp = usrbuf;

    while (nleft &gt; 0)
    {
        if ((nwritten = write(fd, bufp, nleft)) &lt;= 0)
        {
            if (errno == EINTR) /* Interrupted by sig handler return */
                nwritten = 0;   /* and call write() again */
            else
                return -1; /* errno set by write() */
        }
        nleft -= nwritten;
        bufp += nwritten;
    }
    return n;
}
/* $end rio_writen */

/* 
 * rio_read - This is a wrapper for the Unix read() function that
 *    transfers min(n, rio_cnt) bytes from an internal buffer to a user
 *    buffer, where n is the number of bytes requested by the user and
 *    rio_cnt is the number of unread bytes in the internal buffer. On
 *    entry, rio_read() refills the internal buffer via a call to
 *    read() if the internal buffer is empty.
 */
/* $begin rio_read */
static ssize_t rio_read(rio_t *rp, char *usrbuf, size_t n)
{
    int cnt;

    while (rp-&gt;rio_cnt &lt;= 0)
    { /* Refill if buf is empty */
        rp-&gt;rio_cnt = read(rp-&gt;rio_fd, rp-&gt;rio_buf,
                           sizeof(rp-&gt;rio_buf));
        if (rp-&gt;rio_cnt &lt; 0)
        {
            if (errno != EINTR) /* Interrupted by sig handler return */
                return -1;
        }
        else if (rp-&gt;rio_cnt == 0) /* EOF */
            return 0;
        else
            rp-&gt;rio_bufptr = rp-&gt;rio_buf; /* Reset buffer ptr */
    }

    /* Copy min(n, rp-&gt;rio_cnt) bytes from internal buf to user buf */
    cnt = n;
    if (rp-&gt;rio_cnt &lt; n)
        cnt = rp-&gt;rio_cnt;
    memcpy(usrbuf, rp-&gt;rio_bufptr, cnt);
    rp-&gt;rio_bufptr += cnt;
    rp-&gt;rio_cnt -= cnt;
    return cnt;
}
/* $end rio_read */

/*
 * rio_readinitb - Associate a descriptor with a read buffer and reset buffer
 */
/* $begin rio_readinitb */
void rio_readinitb(rio_t *rp, int fd)
{
    rp-&gt;rio_fd = fd;
    rp-&gt;rio_cnt = 0;
    rp-&gt;rio_bufptr = rp-&gt;rio_buf;
}
/* $end rio_readinitb */

/*
 * rio_readnb - Robustly read n bytes (buffered)
 */
/* $begin rio_readnb */
ssize_t rio_readnb(rio_t *rp, void *usrbuf, size_t n)
{
    size_t nleft = n;
    ssize_t nread;
    char *bufp = usrbuf;

    while (nleft &gt; 0)
    {
        if ((nread = rio_read(rp, bufp, nleft)) &lt; 0)
            return -1; /* errno set by read() */
        else if (nread == 0)
            break; /* EOF */
        nleft -= nread;
        bufp += nread;
    }
    return (n - nleft); /* return &gt;= 0 */
}
/* $end rio_readnb */

/* 
 * rio_readlineb - Robustly read a text line (buffered)
 */
/* $begin rio_readlineb */
ssize_t rio_readlineb(rio_t *rp, void *usrbuf, size_t maxlen)
{
    int n, rc;
    char c, *bufp = usrbuf;

    for (n = 1; n &lt; maxlen; n++)
    {
        if ((rc = rio_read(rp, &amp;c, 1)) == 1)
        {
            *bufp++ = c;
            if (c == '\n')
            {
                n++;
                break;
            }
        }
        else if (rc == 0)
        {
            if (n == 1)
                return 0; /* EOF, no data read */
            else
                break; /* EOF, some data was read */
        }
        else
            return -1; /* Error */
    }
    *bufp = 0;
    return n - 1;
}
/* $end rio_readlineb */
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ES使用过程中遇到的坑]]></title>
        <id>https://yangyueren.github.io/post/2/</id>
        <link href="https://yangyueren.github.io/post/2/">
        </link>
        <updated>2019-07-20T16:08:41.000Z</updated>
        <summary type="html"><![CDATA[<p>这是大三暑假完成的一个基于Elasticsearch的搜索引擎的心得篇，十五天的时间遇到了太多的坑，在此做记录。</p>
]]></summary>
        <content type="html"><![CDATA[<p>这是大三暑假完成的一个基于Elasticsearch的搜索引擎的心得篇，十五天的时间遇到了太多的坑，在此做记录。</p>
<!-- more -->
<h2 id="深度搜索引擎体会">深度搜索引擎体会</h2>
<hr>
<pre><code>从7月5号到7月19号，历时十五天的课程结束了，深深的松了一口气，可以缓解一下身心的压力。
这次的项目，是这三年中做的最认真的一次，从5号到18号，每天晚上两点多睡，八点多起床吃早饭来机房，这门课让我半年来不早起不吃早饭的人，养成了吃早饭的习惯，每天来机房时手机的电量有90%，晚上回去的时候还能够剩下65%，每天的上班的六个小时，不熬夜是不可能的，每天晚上都在熬夜补充新的功能，一点点从0到1搭建起自己的系统，用了6台服务器，部署了ES的集群，成果颇丰。
</code></pre>
<p>以上是一些偏离主题的碎碎念，以下是正文。</p>
<p>我们此次项目做的是深度搜索引擎，爬取了小鸡词典、微博、b站等方面的数据，将数据清洗后存到Elasticsearch中，为了充分发挥ES的优势，我使用了三台服务器用来跑ES，一台服务器跑后端程序，另外还有两台图片搜索引擎的服务器，由队友负责搭建。</p>
<p>这个项目的简介可以在此<a href="https://github.com/yangyueren/ChaggieSearchEngine">github</a>仓库中找到，在此不做赘述，本文着重讲述遇到的问题。</p>
<p>首先是Elasticsearch版本的问题，7.2的版本对java Springboot的支持太不友好了，maven里面找不到对7.2版本的ES的repository的插件，所以浪费了一天的时间在这个上面，期间我更换过到2.x的版本和5.x的版本，发现都不如人意，感谢其他组同学的支持，提供了解决思路，使用6.4的版本，最终解决问题。</p>
<p>在ES部署过程中，我一开始就是部署成了集群的状态，对于后续的开发不利，因为我有一台阿里云服务器一直崩溃，所以集群也一直崩溃，而那台服务器又是主节点，在被折磨两天后毅然选择关掉那台服务器，找同学借了其他的服务器。这也拖累了后面的开发进度。如果我一开始就是先在单节点上部署，可以先进行后续的开发，等到最后再来调整服务器集群的问题，这样我在项目的个性化搜索推荐方面还可以做的更加出色。</p>
<p>另一个坑是Springboot对ES的操作，Springboot在定义ES的索引的时候，没办法指定分词器，这就很尴尬。ES的默认分词器是英文的，对于中文的支持特特不友好。我输入的句子，他会拆分成一个个的字建立索引，这样的倒排索引，查一个句子的查询结果与预期的相关度相去甚远，归根结底是Spring自己建立索引时候我不会指定分词器，我最终选择学习ES的语法，首先配置好ES的index，只让Spring插入数据，不让他建索引，这样就解决了这个问题。</p>
<p>另外就是在java中对ES进行数据的查询，repository有些鸡肋，很多想进行的查询都没办法用。我后来查到了一种方法，是加@Query注解，一定程度上缓解了这个问题。其次是sort的问题，我们想做多维的排序，但是ES对这个支持的不是很好，比如有300条数据hit，但是我加入了sort的排序方式，相关度很低的结果可能被拍到前面。于是我在ES中查询时候没有使用sort，但是限制了page size，50左右，然后对返回的一个page的内容进行手动排序，于是手写了多维排序的方法。</p>
<p>为了java中的多维排序，我使用了最愚蠢的方法。我需要对time view like三个维度做多维排序，他们的排列组合有12种（3+6+6），于是我手写了12条下面的语句。</p>
<pre><code class="language-java">/*按照view like time排序*/
Collections.sort(modifyData, (a, b) -&gt; {
                    if (!a.getView().equals(b.getView())) {
                        return b.getView() - a.getView();
                    } else {
                        if (!a.getLike().equals(b.getLike())) {
                            return b.getLike() - a.getLike();
                        } else {
                            if (!a.getTime().equals(b.getTime())) {
                                return compareTime(b.getTime(),a.getTime());
                            }
                        }
                    }
                    return 0;
                });

/*按照like time view排序*/
Collections.sort(modifyData, (a, b) -&gt; {
                    if (!a.getLike().equals(b.getLike())) {
                        return b.getLike() - a.getLike();
                    } else {
                        if (!a.getTime().equals(b.getTime())) {
                            return compareTime(b.getTime(),a.getTime());
                        } else {
                            if (!a.getView().equals(b.getView())) {
                                return b.getView() - a.getView();
                            }
                        }
                    }

                    return 0;
                });
</code></pre>
<p>这是一个比较愚蠢的方法吧，但是确实解决了我的问题，能够解决问题的办法就是好方法！当然了，我也在一直思考如何有更加优美的写法。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chaggie Search Engine]]></title>
        <id>https://yangyueren.github.io/post/1/</id>
        <link href="https://yangyueren.github.io/post/1/">
        </link>
        <updated>2019-07-20T15:58:32.000Z</updated>
        <summary type="html"><![CDATA[<p>这是大三暑假完成的一个基于Elasticsearch的搜索引擎，后端及ES部分由我用Spring boot完成，前端和爬虫由队友完成。</p>
]]></summary>
        <content type="html"><![CDATA[<p>这是大三暑假完成的一个基于Elasticsearch的搜索引擎，后端及ES部分由我用Spring boot完成，前端和爬虫由队友完成。</p>
<!-- more -->
<h2 id="基于elasticsearch集群的数据查询优化">基于Elasticsearch集群的数据查询优化</h2>
<p>Elasticsearch是一个基于Lucene的分布式全文搜索引擎，能够横向扩展数以百计的服务器，存储PB级的数据，而且对每个字段都可以建立索引并且检索，并且可以在极短的时间内存储、搜索和分析大量的数据，程序员最爱的网站Github的搜索就是基于ES构建的，GitHub大约有30TB的索引文件数据，由此可见Elasticsearch（下文简称ES）强大的搜索功能。</p>
<p>在此次的深度搜索引擎项目之中，虽然Elasticsearch也可以在一个节点上使用，该节点可以同时担任master node和data node，但是为了发挥Elasticsearch的分布式搜索的优势，在我们的深度搜索引擎中我们使用了三台服务器提供Elasticsearch的服务。下文将详细介绍从集群部署到优化查询的一些要点。</p>
<h3 id="一-elasticsearch的集群部署">一、Elasticsearch的集群部署</h3>
<h4 id="1-es概念简介">1. ES概念简介</h4>
<p>Elasticsearch中有几个比较重要的概念，集群是指连接在一起的若干台服务器，不同的服务器承担不同的角色，一起提供服务。集群中有主节点、数据节点和客户端节点等。主节点负责管理整个集群，当群集的拓扑结构改变时把索引分片分派到相应的节点上，主节点是从可以担任主节点的节点中选举出来的。数据节点只负责存储数据，客户端节点在选举主节点过程中起作用。ES的分片是把索引信息分散到多个节点上，相当于一桶水用多个杯子装。副本是指索引信息的拷贝。</p>
<p>在进行ES的配置时，首先要考虑节点数和分片数。通过实验，多节点的ES集群中的节点数至少为3，分片数为一倍的节点数量到两倍的节点数量。</p>
<p>节点数和分片数相等时，每个节点负责一个分片的检索，ES集群的性能可以达到最优。对于一个3节点集群，为每个节点分配一个分片，总共3个分片。但是由于ES的不可变性的限制，系统无法对分片进行重新拆分分配，除非重新索引这个文件集合。但是我在三个节点的集群中再加入一个节点，这时候分片数量小于了节点数，在搜索上效率会降低，所以为了支持水平扩展，可以为集群分配比节点数更多的分片数，也就是说每个节点有多个分片。但是每个节点有多个分片时，需要考虑性能的问题，每个节点最好不要超过两个分片，</p>
<p>我采用了官方给默认配置中分片数目为5，这样既可以拓展到5个节点，也可以保证性能。</p>
<p>我的集群的其余配置如下图：</p>
<pre><code class="language-json">#节点集群名称和节点类型
cluster.name: yang-es-clusters
node.name: node-3
node.master: true
node.data: true
#同个集群其他节点的信息，ES通过广播的方式寻找同一集群的其他节点
discovery.zen.ping.unicast.hosts: [ &quot;0.0.0.0&quot;, &quot;106.14.191.xxx&quot;, &quot;120.79.191.xxx&quot;]
#选举主节点时需要由至少2个节点参与投票
discovery.zen.minimum_master_nodes: 2
gateway.recover_after_nodes: 1
#配置本节点的ip，默认开发9300端口用于节点间TCP通信
network.host: 0.0.0.0
network.publish_host: 106.14.227.30
network.bind_host: 0.0.0.0

</code></pre>
<h4 id="2-elsaticsearch集群至少需要有三个节点">2. Elsaticsearch集群至少需要有三个节点</h4>
<p>上文写到我们组搭建的ES集群使用了三台服务器，这也是搭建ES集群所需的最少节点数，是因为需要防止ES集群发生脑裂。ES中维护索引状态最重要的节点是主节点，主节点是被投票选举出来的。</p>
<h5 id="三个和尚投票">三个和尚投票</h5>
<p>当主节点出现问题，从节点不能与主节点通信时，从节点会发起选举任命新的主节点，同时新的主节点会接管旧的主节点的所有工作，如果旧的主节点重新恢复并加入到集群中，新的主节点会将原来旧的主节点降级为从节点，这样就不会有冲突发生。所有这个过程都由ES自己处理，使用者无需任何参与。</p>
<h5 id="两个和尚投票">两个和尚投票</h5>
<p>但是，当只有两个节点的时候，一主（master）一从（slave），如果主从直接的通信出现问题时，从节点slave会自我提升为master，但是当恢复通信时，我们就会同时有两个master。因为此时，对于原来的主节点角度考虑，它认为是原来的从节点出现问题，现在仍然需要作为slave重新加入。这样，两个节点的时候，我们就出现了集群不知道将哪个节点选举为主节点的情况，也就是我们通常说的“分脑”。</p>
<p>为了防止这种情况的发生，第三个节点的出现会打破平衡，解决冲突问题。</p>
<h5 id="三个和尚仍然存在问题">三个和尚仍然存在问题</h5>
<p>分脑的问题同样会出现在具有三或三个以上节点的集群中，为了降低发生的概率，ElasticSearch提供了一个配置 <code>discovery.zen.minimum_master_nodes</code>它规定了在选举新的master时，一个集群下最少需要的节点数。例如，一个3节点集群，这个数字为2，2个节点可以防止单个节点在脱离集群时，将其自己选举成master，相反，它会等待直到重新加入到集群中。这个数值可以通过一个公式确定：</p>
<p>这里的配置是指当主节点宕掉掉时候至少同时需要几个节点才重新进行投票选举新的主节点，官方建议将此数目配置为<code>N / 2 + 1</code>，可以有效的防止脑裂。</p>
<pre><code class="language-json">discovery.zen.minimum_master_nodes: 2
</code></pre>
<p>从图中分析可以得知，如果只有两个节点，当这两个节点通讯故障的时候，会各自选举自己为主节点，而当通讯恢复正常时候会发生冲突，这与区块链的思想不谋而合，只有控制了51%以上的节点，才可以掌控整个集群。</p>
<p>我在配置ES的时候，主节点所在的服务器由于网络问题，经常会发生断网的现象，此时集群的状态会由绿色（正常）转变为红色（预警）状态，而当非主节点宕机的时候，集群状态会变为黄色（所有的主分片可用，但是副本分片不可用）。这个问题的解决方案只有一种，设置容易宕机的节点为数据节点，禁止其被选举为主节点。</p>
<h3 id="二-elasticsearch的查询参数优化">二、Elasticsearch的查询参数优化</h3>
<h4 id="1-lucene的打分模型">1. Lucene的打分模型</h4>
<figure data-type="image" tabindex="1"><img src="http://www.biaodianfu.com/wp-content/uploads/2016/09/lucene-tf-idf.png" alt="lucene-tf-idf" loading="lazy"></figure>
<p>由于ES是基于Lucene，所以ES也是使用的打分机制。通过上面的公式，一篇文档的分数实际上是由查询语句q和文档d作为变量的一个函数值。打分公式中有两部分不直接依赖于查询词，它们是coord和queryNorm。公式的值是这样计算的，coord和queryNorm两大部分直接乘以查询语句中每个查询词计算值的总和。另一方面，这个总和也是由每个查询词的词频(tf)，逆文档频率(idf)，查询词的权重，还有norm，也就是前面说的length norm相乘而得的结果。</p>
<p>从中可以得出以下几条规则：</p>
<ul>
<li>匹配到的关键词越稀有，文档的得分就越高。</li>
<li>文档的域越小(包含比较少的Term)，文档的得分就越高。</li>
<li>设置的权重(索引和搜索时设置的都可以)越大，文档得分越高。</li>
</ul>
<p>随着Lucene的发展，打分模型也引入了新的相似度模型，并且可以在ES中指定，现在比较流行的是Okapi BM25，Divergence from randomness和Information based。</p>
<p>BM25是基于概率模型的相似度模型，适合处理短文本，关键词的重复次数对整个文档得分影响比较大。DFR和IB比较类似，基于同名概率模型，适用于自然语言类的文本。我们的搜索引擎要搜索的字段比较少，内容也是以短文本为主，并且倾向于能够对名字和标签进行准确匹配，如果关键词在内容中多次重复，明显词条是用户所查询的结果，所以BM25更加适合我们的搜索引擎。</p>
<h4 id="2-分词器的选择">2. 分词器的选择</h4>
<p>ES是基于词的搜索引擎，其能够快速的通过搜索词检索出对应的文章归功于倒排索引，使用不同的分词器对于检索效果也有重大影响。</p>
<p>ES的默认分词器对英文句子的切割效果比较好，但用于中文句子的分割时，只会将句子分割成孤立的一个个的字，所以需要指定建立索引时的分词器和搜索分词器。我们使用的是IKAnalyzer，是目前比较流行的中文分词器之一,设置比较简单,稳定。</p>
<p>在Sprint Boot中建立索引时候，对ES的支持度不如直接在ES里面自己建立索引可操作性高，以下是我建立索引的代码。</p>
<pre><code class="language-json">curl -XPOST http://106.14.227.30:9200/chageng/EntryDb/_mapping -H 'Content-Type:application/json' -d' { &quot;EntryDb&quot;: { &quot;properties&quot;: {
    &quot;content&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;analyzer&quot;: &quot;ik_max_word&quot;,
        &quot;search_analyzer&quot;: &quot;ik_smart&quot;,
        &quot;fields&quot;: {
            &quot;keyword&quot;: {
                &quot;type&quot;: &quot;text&quot;,
                &quot;analyzer&quot;: &quot;ik_max_word&quot;,
                &quot;search_analyzer&quot;: &quot;ik_smart&quot;
            }
        }
    },
    &quot;imageList&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;fields&quot;: {
            &quot;keyword&quot;: {
                &quot;type&quot;: &quot;keyword&quot;,
                &quot;ignore_above&quot;: 256
            }
        }
    },
    &quot;like&quot;: {
        &quot;type&quot;: &quot;long&quot;
    },
    &quot;name&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;analyzer&quot;: &quot;ik_max_word&quot;,
        &quot;search_analyzer&quot;: &quot;ik_smart&quot;,
        &quot;fields&quot;: {
            &quot;keyword&quot;: {
                &quot;type&quot;: &quot;text&quot;,
                &quot;analyzer&quot;: &quot;ik_max_word&quot;,
                &quot;search_analyzer&quot;: &quot;ik_smart&quot;
            }
        }
    }
}'
</code></pre>
<h4 id="3-查询语句的优化">3. 查询语句的优化</h4>
<h5 id="31-term-match与multi_match">3.1 term、match与multi_match</h5>
<p>ES中的term是代表完全匹配，也就是精确查询，搜索前不会再对搜索词进行分词，所以我们的搜索词必须是文档分词集合中的一个。以下代码将会在name中精确匹配为“小鸡快跑”的词条。</p>
<pre><code class="language-json">curl -XPOST '106.14.227.30: 9200/chageng/_search?pretty' -H 'Content-Type: application/json' -d '
{
  &quot;query&quot;:{
    &quot;term&quot;:{
        &quot;name&quot;:&quot;小鸡快跑&quot;
    }
  }
}'
</code></pre>
<p>ES的match搜索会先对搜索词进行分词，对于最基本的match搜索来说，只要搜索词的分词集合中的一个或多个存在于文档中即可，例如，当我们搜索<code>小鸡快跑</code>，搜索词会先分词为<code>小鸡</code>和<code>快跑</code>,只要文档中包含<code>小鸡</code>和<code>快跑</code>任意一个词，都会被搜索到。</p>
<p>如果文档1中有<code>小鸡</code>，文档2中有<code>快跑</code>，那么这两个文档都会被检索到，而如果文档3中有<code>小鸡</code>和<code>快跑</code>两个词，文档3也将被返回，并且文档3将被排在首位。所有被返回的文档将依靠_score的分数进行排序，得分的算法参考上文。</p>
<pre><code class="language-json">curl -XPOST '106.14.227.30: 9200/chageng/_search?pretty' -H 'Content-Type: application/json' -d '
{
    &quot;query&quot;: {
        &quot;match&quot;: {
            &quot;content&quot;: &quot;小鸡快跑&quot;
        }
    }
}'
</code></pre>
<p>ES的multi_match是对多个字段进行匹配，其中一个字段包含分词，该文档即可被搜索到并且返回。在实际使用中用的比较多。</p>
<pre><code class="language-json">curl -XPOST '106.14.227.30: 9200/chageng/_search?pretty' -H 'Content-Type: application/json' -d '
{
&quot;query&quot;: {
&quot;bool&quot;: {
    &quot;must&quot;: {
        &quot;match&quot;: {
            &quot;tagList&quot;: &quot;社交&quot;
        }
    },
    &quot;should&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;: &quot;吓得我瓜子都掉了&quot;,
            &quot;type&quot;: &quot;best_fields&quot;,
            &quot;fields&quot;: [
                &quot;name^2&quot;,
                &quot;content&quot;
            ],
	          &quot;fuzziness&quot;: &quot;AUTO&quot;,
            &quot;tie_breaker&quot;: 0.4,
            &quot;minimum_should_match&quot;: &quot;30%&quot;
        }
    }
}
}
}
'
</code></pre>
<h5 id="32-组合过滤器bool">3.2 组合过滤器bool</h5>
<p><code>bool</code> 过滤器通过 <code>and</code> 、 <code>or</code> 和 <code>not</code> 逻辑组合将多个过滤器进行组合。<code>bool</code> 查询可以接受 <code>must</code> 、 <code>must_not</code> 和 <code>should</code> 参数下的多个查询语句，对查询结果进行筛选，分别对应<code>AND NOT OR</code>。<code>bool</code> 查询会为每个文档计算相关度评分 <code>_score</code> ， 再将所有匹配的 <code>must</code> 和 <code>should</code> 语句的分数 <code>_score</code> 求和，最后除以 <code>must</code> 和 <code>should</code> 语句的总数。</p>
<p><code>must_not</code> 语句不会影响评分； 它的作用只是将不相关的文档排除。</p>
<p><code>should</code>过滤的数量是由<code>minimum_should_match</code>参数来进行控制，该参数可以是百分比，也可以是一个数字，我在多次实验后发现40%的效果最好。</p>
<p>以下是bool的基本用法。</p>
<pre><code class="language-json">curl -XPOST '106.14.227.30: 9200/chageng/_search?pretty' -H 'Content-Type: application/json' -d '
{
&quot;query&quot;: {
&quot;bool&quot;: {
    &quot;must&quot;: {
        &quot;match&quot;: {
            &quot;tagList&quot;: &quot;游戏&quot;
        }
    },
    &quot;should&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;: &quot;秦王&quot;,
            &quot;type&quot;: &quot;best_fields&quot;,
            &quot;fields&quot;: [
                &quot;name^5&quot;,
              	&quot;tagList^2&quot;,
                &quot;content^1&quot;
            ],
            &quot;tie_breaker&quot;: 0.4,
            &quot;minimum_should_match&quot;: &quot;40%&quot;
        }
    },
    &quot;filter&quot;: {
        &quot;range&quot;: {
            &quot;time&quot;: {
                &quot;gte&quot;: &quot;2012-09-09&quot;,
                &quot;lt&quot;: &quot;2019-09-09&quot;
            }
        }
    }
}
}
}'
</code></pre>
<h5 id="33-boost权重控制">3.3 boost权重控制</h5>
<p>在多字段匹配中，我在name tagList 和 content字段中对内容进行查询，但是想让name字段拥有有更高的权重，可以通过指定 <code>boost</code> 来控制任何查询语句的相对的权重， <code>boost</code> 的默认值为 <code>1</code> ，大于 <code>1</code> 会提升一个语句的相对权重。基本使用见上条ES语句。</p>
<p>基于 TF/IDF 的评分模型中，如果使用了<code>boost</code>改变权重，新的评分 <code>_score</code> 会在应用权重提升之后进行归一化处理 ，并不是线性的变化。</p>
<h5 id="34-模糊匹配">3.4 模糊匹配</h5>
<p>模糊查询的工作原理是给定原始词项及构造一个编辑自动机— 像表示所有原始字符串指定编辑距离的字符串的一个大图表。然后模糊查询使用这个自动机依次高效遍历词典中的所有词项以确定是否匹配。 一旦收集了词典中存在的所有匹配项，就可以计算匹配文档列表。在搜索巨大文档时候，模糊匹配的效率很低，故可以用以下两个参数限制对性能的影响，prefix_length为不能被 “模糊化” 的初始字符数，建议设置为了3，max_expansions限制产生的模糊选项的总数量。</p>
<pre><code class="language-json">curl -XPOST '106.14.227.30: 9200/chageng/_search?pretty' -H 'Content-Type: application/json' -d '
{
    &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;: &quot;吓得我瓜子都掉了&quot;,
            &quot;type&quot;: &quot;best_fields&quot;,
            &quot;fields&quot;: [
                &quot;name^2&quot;,
                &quot;content&quot;
            ],
            &quot;fuzziness&quot;: &quot;AUTO&quot;,
            &quot;tie_breaker&quot;: 0.4,
            &quot;minimum_should_match&quot;: &quot;40%&quot;
        }
    }
}'
</code></pre>
<h5 id="35-随机评分">3.5 随机评分</h5>
<p>我们的搜索词条结果集中有很多点赞数一样的词条，在指定按点赞数排序这种方式后，有相同评分 <code>_score</code> 的文档会每次都以相同次序出现，为了提高展现率，可以引入一些随机性，保证有相同评分的文档都能有均等相似的展现机率。</p>
<p>每个用户看到不同的随机次序，但也同时希望如果是同一用户翻页浏览时，结果的相对次序能始终保持一致。这种行为被称为 一致随机（consistently random） 。</p>
<p>引用：https://www.elastic.co/guide/cn/elasticsearch/guide/current/random-scoring.html</p>
<p>以下是样例：</p>
<pre><code class="language-json">curl -XPOST '106.14.227.30: 9200/chageng/_search?pretty' -H 'Content-Type: application/json' -d '
{
  &quot;query&quot;: {
    &quot;function_score&quot;: {
      &quot;filter&quot;: {
        &quot;match&quot;: { &quot;name&quot;: &quot;小鸡快跑&quot; }
      },
      &quot;functions&quot;: [
        {
          &quot;filter&quot;: { &quot;term&quot;: { &quot;tagList&quot;: &quot;小鸡&quot; }},
          &quot;weight&quot;: 1
        },
        {
          &quot;filter&quot;: { &quot;term&quot;: { &quot;tagList&quot;: &quot;游戏&quot; }},
          &quot;weight&quot;: 2
        },
        {
          &quot;random_score&quot;: { 
            &quot;seed&quot;:  &quot;the users session id&quot; 
          }
        }
      ],
      &quot;score_mode&quot;: &quot;sum&quot;
    }
  }
}
</code></pre>
<h3 id="三-elasticsearch性能问题">三、Elasticsearch性能问题</h3>
<h4 id="1-数据预热">1. 数据预热:</h4>
<p>ES可以在查询前进行预热，将查询中十分依赖的字段的数据加载出来，可以使用Elasticsearch为类型和索引定义预热查询。</p>
<p>定义一个新的预热查询，和普通查询没什么区别，只是它存储在Elasticsearch一个特殊的名为_warmer的索引中，以下是我的预热查询。</p>
<pre><code class="language-json">curl -XPUT '106.14.227.30: 9200/chageng/_warmer?pretty' -H 'Content-Type: application/json' -d '
{
    &quot;query&quot;: {
        &quot;match_all&quot;: {}
    },
    &quot;facets&quot;: {
        &quot;warming_facet&quot;: {
            &quot;terms&quot;: {
                &quot;field&quot;: &quot;name&quot;
            }
        }
    }
}'
</code></pre>
<h4 id="2-优化索引">2. 优化索引</h4>
<p>我建立的索引，每个词条都包含较多的内容，不仅包括了该词条的基本信息（name，tag，content，view，like），还包括了从微博、B站、谷歌等地方爬取到的相关信息，每个词条中包含的数据比较多，对完整的词条建立索引，每次的查询速度与在mongodb里面检索持平，在做自动补全时能够有肉眼可见的延迟。</p>
<p>出现查询速度过慢的情况有两方面的原因。第一是建立的索引包含的内容过多，比如微博、B站的数据大约是该词条的基本信息的25倍以上，而这些微博、B站的信息我们在做检索的时候并不需要对这些字段进行检索，这些无效的信息拖累了检索速度。第二是网络传输延迟，因为我们的ES集群和我们的搜索引擎服务并不在同一个机器上面，他们之间是通过网络进行通信，由于每条词条包含数据比较大，所以如果查询结果中有上百条的数据被命中，返回这些数据时需要比较多的时间。</p>
<p>我们的解决方案是建立两个索引，第一个索引存储词条的基本信息，第二个索引存储词条的所有信息，但使用检索功能的时候，我们只需要在第一个索引中检索，将词条的基本信息返回呈现给用户，这样可以大大加快速度。为了将速度优化到极致，在不影响用户正常使用的情况下，我们又对搜索结果的数量进行了限制，每次只返回当前页面要展现的搜索条目，最快的呈现给用户结果，其余的搜索结果用异步的方式加载。当用户进入词条详细页面时，我们可以通过该词条的id，到ES中的第二个索引中查找该id词条的所有信息进行返回，这样的检索速度能够提升到28倍。</p>
<h4 id="3-优化存储">3. 优化存储</h4>
<p>上文提到索引可以存储到ES中，这样的查询效率最高，那具体的数据可以存储到MySQL、MongoDb，ES三种数据库中，考虑到我们的数据类型以json文件为主，MySQL需要建立多张表来实现关系间的映射，故不做考虑。</p>
<p>我对MongoDb存储词条的所有信息，与直接用ES存储所有的信息进行检索做了一个对比，发现两者在检索10000条数据运行时间上并没有太大差异，所以直接使用ES进行存储了所有的数据。</p>
]]></content>
    </entry>
</feed>